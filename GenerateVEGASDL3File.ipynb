{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline --no-import-all\n",
    "\n",
    "from ctypes import c_float\n",
    "\n",
    "# import numpy as np\n",
    "from astropy.io import fits\n",
    "\n",
    "\n",
    "import ROOT\n",
    "from ROOT import gSystem, TFile, TGraphAsymmErrors\n",
    "\n",
    "\n",
    "from root_numpy import hist2array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if gSystem.Load(\"libVEGASCommon.dylib\"):\n",
    "    print(\"Problem loading VEGAS Common libraries - please check this before proceeding\")\n",
    "if gSystem.Load(\"libVEGASStage6.dylib\"):\n",
    "    print(\"Problem loading VEGAS Stage 6 libraries - please check this before proceeding\")\n",
    "if gSystem.Load(\"libVEGASStage5.dylib\"):\n",
    "    print(\"Problem loading VEGAS Stage 5 libraries - please check this before proceeding\")\n",
    "from ROOT import VARootIO, VAEffectiveAreaManager, VAEASimpleParameterData\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compatability\n",
    "Works with VEGAS v2.5.7 or later.\n",
    "\n",
    "# Data format\n",
    "the data format is defined at : https://gamma-astro-data-formats.readthedocs.io/en/latest/\n",
    "\n",
    "# Task List\n",
    "\n",
    "## Top priorities\n",
    "* Time Cuts from ROOT file\n",
    "* Average Azimuth assumes southerly source ....\n",
    "* Convert to format so can read through a stage 6 runlist file\n",
    "* Check validity of 30 min runs for EA (compare decent Crab runlist in 30 vs 10 min runs @ different zenith angles).\n",
    "* Adding ED capability.\n",
    "\n",
    "## Validation\n",
    "* Check high stats crab spectra\n",
    "* Check other spectra\n",
    "* Check significances/sky maps\n",
    "* Upper limits\n",
    "\n",
    "## Documentation\n",
    "* Save a number of notebooks that show how to do conversion and analysis\n",
    "\n",
    "## Wish list\n",
    "* All offset - this will require reworking events list for saving noises etc.\n",
    "* Event Types - how do spectra compare when produced as \"all events\" vs. breaking up into 2, 3 and 4 tel events.\n",
    "* Get window size for noise from root/ea file - at the moment assume it is 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowSizeForNoise = 7\n",
    "\n",
    "def decodeConfigMask(mask=15):\n",
    "    '''Decode the telescope config mask to find the telescpes in the array'''\n",
    "    tels = []\n",
    "    if mask >= 8:\n",
    "        tels.append(4)\n",
    "        mask -= 8\n",
    "    if mask >= 4:\n",
    "        tels.append(3)\n",
    "        mask -= 4\n",
    "    if mask >= 2:\n",
    "        tels.append(2)\n",
    "        mask -= 2\n",
    "    if mask >= 1:\n",
    "        tels.append(1)\n",
    "        mask -= 1\n",
    "    return sorted(tels)\n",
    "\n",
    "\n",
    "def produceTelList(mask):\n",
    "    '''Convert the list of telescopes into a string for FITS header'''\n",
    "    telList = \"\"\n",
    "    for tel in decodeConfigMask(mask):\n",
    "        telList += \"T\" + str(tel) + \",\"\n",
    "    return telList[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Generation\n",
    "\n",
    "## First we need to generate the primary HDU\n",
    "\n",
    "This contains the information about who wrote the file and the standards that it is written too.\n",
    "The basic header has some information, we need to complete it to have the following\n",
    "\n",
    "<pre>\n",
    "SIMPLE  =                    T / file does conform to FITS standard             \n",
    "BITPIX  =                    8 / number of bits per data pixel                  \n",
    "NAXIS   =                    0 / number of data axes                            \n",
    "EXTEND  =                    T / FITS dataset may contain extensions            \n",
    "COMMENT   FITS (Flexible Image Transport System) format is defined in 'Astronomy\n",
    "COMMENT   and Astrophysics', volume 376, page 359; bibcode: 2001A&A...376..359H \n",
    "TELESCOP= 'VERITAS'            / Telescope                                      \n",
    "LICENSE = '        '           / Copyright (c) 2018,The VERITAS Collaboration     \n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 3\n",
    "\n",
    "runs = [54809, 57993, 58456, 59523]\n",
    "atms = [21, 22, 21, 21]\n",
    "\n",
    "run = str(runs[i])\n",
    "atm = str(atms[i])\n",
    "st5File = \"VEGAS/\"+run+\".med.ED.050.St5_Stereo.root\"\n",
    "eaFile = \"VEGAS/EA_na\"+atm+\"stan_medPoint_050_ED_GRISU.root\"\n",
    "outfile = 'VEGAS/DL3/'+run+'_DL3.fits'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdu0 = fits.PrimaryHDU()\n",
    "hdu0.header.set('TELESCOP', 'VERITAS', 'Telescope')\n",
    "hdu0.header.set('LICENSE ', '', 'Copyright (c) 2018,The VERITAS Collaboration')\n",
    "hdu0.header['COMMENT'] = \"FITS (Flexible Image Transport System) format is defined in 'Astronomy\"\n",
    "hdu0.header['COMMENT'] = \"and Astrophysics', volume 376, page 359; bibcode: 2001A&A...376..359H\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdu0.header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Event Table \n",
    "The second hdu is the event table - this includes all of the events that pass the gamma/hadron selection cuts.\n",
    "\n",
    "To load this we need to read in the VEGAS stage 5 file and select the keys of interest.  Then this can be saved into a table and then the hdu.  \n",
    "\n",
    "We also need to put a lot of information into the header about the observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vegasFileIO = VARootIO(st5File, True)\n",
    "runHeader = vegasFileIO.loadTheRunHeader()\n",
    "selectedEventsTree = vegasFileIO.loadTheCutEventTree()\n",
    "qStatsData = vegasFileIO.loadTheQStatsData()\n",
    "\n",
    "arrayInfo = vegasFileIO.loadTheArrayInfo(0)\n",
    "pixelData = vegasFileIO.loadThePixelStatusData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vegasFileIO.loadTheRunHeader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runHeader.printRunHeader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selectedEventsTree.Print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First we need to generate an array of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evNumArr = []\n",
    "timeArr = []\n",
    "raArr = []\n",
    "decArr = []\n",
    "azArr = []\n",
    "altArr = []\n",
    "energyArr = []\n",
    "detXArr = []\n",
    "detYArr = []\n",
    "nTelArr = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we can populate the array from the ROOT file\n",
    "\n",
    "Note: We also need to save some useful information for the average altitude, azimuth, RA and Dec.\n",
    "\n",
    "I am not sure how best to do the average of the azimuth - I have ignored this for now ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avAlt = []\n",
    "avAz = []\n",
    "avRA = []\n",
    "avDec = []\n",
    "for ev in selectedEventsTree:\n",
    "    evNumArr.append(ev.S.fArrayEventNum)\n",
    "    timeArr.append(float(ev.S.fTime.getDayNS())/1e9)\n",
    "    raArr.append(np.rad2deg(ev.S.fDirectionRA_J2000_Rad))\n",
    "    decArr.append(np.rad2deg(ev.S.fDirectionDec_J2000_Rad))\n",
    "    azArr.append(np.rad2deg(ev.S.fDirectionAzimuth_Rad))\n",
    "    altArr.append(np.rad2deg(ev.S.fDirectionElevation_Rad))\n",
    "    energyArr.append(ev.S.fEnergy_GeV / 1000.)\n",
    "    detXArr.append(ev.S.fDirectionXCamPlane_Deg)\n",
    "    detYArr.append(ev.S.fDirectionYCamPlane_Deg)\n",
    "    nTelArr.append(ev.S.fImages)\n",
    "    \n",
    "    avAlt.append(ev.S.fArrayTrackingElevation_Deg)\n",
    "    avAz.append(ev.S.fArrayTrackingAzimuth_Deg)\n",
    "    avRA.append(ev.S.fArrayTrackingRA_J2000_Rad)\n",
    "    avDec.append(ev.S.fArrayTrackingDec_J2000_Rad)\n",
    "        \n",
    "avAlt = np.mean(avAlt)\n",
    "# Calculate average azimuth angle from average vector on a circle\n",
    "# https://en.wikipedia.org/wiki/Mean_of_circular_quantities\n",
    "avAz = np.rad2deg(np.arctan2(np.sum(np.sin(avAz)),np.sum(np.cos(avAz))))\n",
    "avAz = avAz if avAz > 0 else avAz + 360\n",
    "\n",
    "avRA = np.rad2deg(np.mean(avRA))\n",
    "avDec = np.rad2deg(np.mean(avDec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we need to generate an HDU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdu1 = fits.BinTableHDU.from_columns([\n",
    "    fits.Column(name='EVENT_ID', format='1J', array=evNumArr), \n",
    "    fits.Column(name='TIME', format='1D', array=timeArr, unit=\"s\"), \n",
    "    fits.Column(name='RA', format='1E', array=raArr, unit = \"deg\"), \n",
    "    fits.Column(name='DEC', format='1E', array=decArr, unit = \"deg\"), \n",
    "    fits.Column(name='ALT', format='1E', array=altArr, unit = \"deg\"), \n",
    "    fits.Column(name='AZ', format='1E', array=azArr, unit = \"deg\"), \n",
    "    fits.Column(name='ENERGY', format='1E', array=energyArr, unit = \"TeV\"), \n",
    "    fits.Column(name='DETX', format='1E', array=detXArr, unit = \"deg\"), \n",
    "    fits.Column(name='DETY', format='1E', array=detYArr, unit = \"deg\"),\n",
    "    fits.Column(name=\"EVENT_TYPE\", format=\"1J\", array=nTelArr)\n",
    "])\n",
    "hdu1.name = \"EVENTS\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Header Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdu1.header.set('OBS_ID  ', runHeader.getRunNumber(), 'Run Number')\n",
    "hdu1.header.set('TELESCOP', 'VERITAS', 'Data from VERITAS')\n",
    "\n",
    "startTime = runHeader.getStartTime()\n",
    "endTime = runHeader.getEndTime()\n",
    "\n",
    "startTime_s = float(startTime.getDayNS()) / 1e9\n",
    "endTime_s = float(endTime.getDayNS()) / 1e9\n",
    "\n",
    "print(startTime_s,endTime_s)\n",
    "startTime_s = float(startTime.getDayNS()) / 1e9\n",
    "endTime_s = float(endTime.getDayNS()) / 1e9\n",
    "hdu1.header.set('DATE-OBS',\n",
    "                startTime.getString().split()[0],\n",
    "                'start date (UTC) of obs yy-mm-dd')\n",
    "hdu1.header.set('TIME-OBS',\n",
    "                startTime.getString().split()[1],\n",
    "                'start time (UTC) of obs hh-mm-ss')\n",
    "hdu1.header.set('DATE-END',\n",
    "                endTime.getString().split()[0],\n",
    "                'end date (UTC) of obs yy-mm-dd')\n",
    "hdu1.header.set('TIME-END',\n",
    "                endTime.getString().split()[1],\n",
    "                'end time (UTC) of obs hh-mm-ss')\n",
    "\n",
    "hdu1.header.set('TSTART  ',\n",
    "                startTime_s,\n",
    "                'mission time of start of obs [s]')\n",
    "hdu1.header.set('TSTOP   ',\n",
    "                endTime_s,\n",
    "                'mission time of end of obs [s]')\n",
    "hdu1.header.set('MJDREFI ',\n",
    "                int(startTime.getMJDInt()), 'int part of reference MJD [days]')\n",
    "hdu1.header.set('MJDREFF ', 0., 'fractional part of reference MJD [days]')\n",
    "\n",
    "hdu1.header.set('TIMEUNIT', 's', 'time unit is seconds since MET start')\n",
    "hdu1.header.set('TIMESYS ', 'utc', 'time scale is UTC')\n",
    "hdu1.header.set('TIMEREF ', 'local', 'local time reference')\n",
    "\n",
    "hdu1.header.set('ONTIME  ', \n",
    "                endTime_s - startTime_s,\n",
    "                'time on target (including deadtime)')\n",
    "hdu1.header.set('LIVETIME', runHeader.pfRunDetails.fRunNominalLiveTimeSeconds,\n",
    "                '(dead=ONTIME-LIVETIME) [s] ')\n",
    "hdu1.header.set('DEADC   ', runHeader.getLiveTimeFrac(),\n",
    "                'average deadtime fraction [] ')\n",
    "\n",
    "hdu1.header.set('OBJECT  ', runHeader.getSourceId(), 'observed object')\n",
    "\n",
    "hdu1.header.set('RA_PNT  ', avRA, 'observation position RA [deg]')\n",
    "hdu1.header.set('DEC_PNT ', avDec, 'observation position DEC [deg]')\n",
    "hdu1.header.set('ALT_PNT ', avAlt, 'average altitude of pointing [deg]')\n",
    "hdu1.header.set('AZ_PNT  ', avAz, 'average azimuth of pointing [deg]')\n",
    "\n",
    "hdu1.header.set('RA_OBJ  ',\n",
    "                np.rad2deg(runHeader.getSourceRA()),\n",
    "                'observation position RA [deg]')\n",
    "hdu1.header.set('DEC_OBJ ',\n",
    "                np.rad2deg(runHeader.getSourceDec()),\n",
    "                'observation position DEC [deg]')\n",
    "\n",
    "# get the list of telescopes that participate in the event\n",
    "hdu1.header.set('TELLIST',\n",
    "                produceTelList(runHeader.fRunInfo.fConfigMask),\n",
    "                'comma-separated list of tel IDs')\n",
    "hdu1.header.set('N_TELS', runHeader.pfRunDetails.fTels,\n",
    "                'number of telescopes in event list')\n",
    "\n",
    "# other info - weather? pointing mode\n",
    "\n",
    "hdu1.header.set('EUNIT   ', 'TeV', 'energy unit')\n",
    "hdu1.header.set('GEOLAT  ', np.rad2deg(arrayInfo.longitudeRad()), 'longitude of array center [deg]')\n",
    "hdu1.header.set('GEOLON  ', np.rad2deg(arrayInfo.latitudeRad()), 'latitude of array center [deg]')\n",
    "hdu1.header.set('ALTITUDE', arrayInfo.elevationM(), 'altitude of array center [m]')\n",
    "\n",
    "# What are these for? - May note be needed, leave out for now.\n",
    "# hdu1.header.set('DSTYP1', 'TIME    ', 'Data selection type')\n",
    "# hdu1.header.set('DSUNI1', 's       ', 'Data selection unit')\n",
    "# hdu1.header.set('DSVAL1', 'TABLE   ', 'Data selection value')\n",
    "# hdu1.header.set('DSREF1', ':GTI    ', 'Data selection reference')\n",
    "# hdu1.header.set('DSTYP2', 'POS(RA,DEC)', 'Data selection type')\n",
    "# hdu1.header.set('DSUNI2', 'deg     ', 'Data selection unit')\n",
    "# hdu1.header.set('DSVAL2', 'CIRCLE(83.63,22.01,5)', 'Data selection value')\n",
    "# hdu1.header.set('DSTYP3', 'ENERGY  ', 'Data selection type')\n",
    "# hdu1.header.set('DSUNI3', 'TeV     ', 'Data selection unit')\n",
    "# hdu1.header.set('DSVAL3', '0.05:100', 'Data selection value')\n",
    "# hdu1.header.set('NDSKEYS', '3       ', 'Number of data selections')\n",
    "# hdu1.header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Good Time Intervals (GTI)\n",
    "\n",
    "### Header\n",
    "<pre>\n",
    "TSTART  =                      /start time same unit and system used in\n",
    "                                the rate table\n",
    "TSTOP   =                      /stop time same unit and system used in\n",
    "                                the rate table\n",
    "TIMEZERO=                      /zero time same unit and system used\n",
    "                                in the rate table\n",
    "TTYPE1  = 'START   '           / start of good time interval\n",
    "TTYPE2  = 'STOP    '           / end of good time interval\n",
    "EXTNAME = 'GTI     '           / name: Good Time Intervals\n",
    "</pre>\n",
    "\n",
    "### To-Do\n",
    "1. Check that this is where the time cuts come from\n",
    "2. exposure ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse time cut from cut config text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse time cut string \n",
    "def parseTimeCut(tCutStr):\n",
    "    cut_arr = []\n",
    "    for cc in tCutStr.split(','):\n",
    "        start = float(cc.split('/')[0])\n",
    "        end   = float(cc.split('/')[1])\n",
    "        cut_arr.append((start,end))\n",
    "    return cut_arr\n",
    "\n",
    "# Find time cut string\n",
    "def getTimeCut(config_str_ori):\n",
    "    config_str = str(config_str_ori)\n",
    "    for i in config_str.splitlines():\n",
    "        # Skip comment lines\n",
    "        if( (len(i) ==0) or (i.strip()[0] == '#')):\n",
    "            continue\n",
    "        # I\n",
    "        elif(i.find('ES_CutTimes') >= 0 ):\n",
    "            key,cut_str = i.split(' ')\n",
    "            if(len(cut_str) ==0):\n",
    "                return parseTimeCut('0/0')\n",
    "            return parseTimeCut(cut_str)\n",
    "        \n",
    "# Check if two cuts can be merged\n",
    "def isMergable(cut1,cut2):\n",
    "    s1,e1 = cut1\n",
    "    s2,e2 = cut2\n",
    "    if(s1 > s2):\n",
    "        s1,e1 = cut2\n",
    "        s2,e2 = cut1\n",
    "    return (s2 <= e1)\n",
    "\n",
    "# Merge two cuts\n",
    "def mergeTwoTimeCut(cut1,cut2):\n",
    "    s1,e1 = cut1\n",
    "    s2,e2 = cut2\n",
    "    if(s1 > s2):\n",
    "        s1,e1 = cut2\n",
    "        s2,e2 = cut1\n",
    "    return (s1,max(e1,e2))\n",
    "\n",
    "\n",
    "# Merge time cuts if there are overlaps\n",
    "def mergeTimeCut(cuts):\n",
    "    cut_sorted = sorted(cuts)\n",
    "    merged_cut = []\n",
    "    while(len(cut_sorted) > 0):\n",
    "        test = cut_sorted[0]\n",
    "        cut_sorted = cut_sorted[1:]\n",
    "        the_rest = []\n",
    "        for cc in cut_sorted:\n",
    "            if(isMergable(test,cc)):\n",
    "                test = mergeTwoTimeCut(test,cc)\n",
    "            else:\n",
    "                the_rest.append(cc)\n",
    "        \n",
    "        cut_sorted = the_rest\n",
    "        merged_cut.append(test)\n",
    "    return  list(filter(lambda x: x[0] != x[1],merged_cut))\n",
    "\n",
    "# Get Good Time array from cuts\n",
    "def getGTArray(startTime_s,endTime_s,cuts):\n",
    "    if(len(cuts) ==0):\n",
    "        return np.array([startTime_s]),np.array([endTime_s])\n",
    "    goodTimeStart =  [startTime_s] + [ cc[1]+startTime_s for cc in cuts]\n",
    "    goodTimeStop = [ cc[0]+startTime_s for cc in cuts] + [endTime_s]\n",
    "\n",
    "    if(goodTimeStop[-1] >endTime_s):\n",
    "        goodTimeStop[-1] = endTime_s\n",
    "    if(goodTimeStart[-1] > goodTimeStop[-1]):\n",
    "        return goodTimeStart[:-1],goodTimeStop[:-1]\n",
    "    return np.array(goodTimeStart),np.array(goodTimeStop)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "runHeader = vegasFileIO.loadTheRunHeader()\n",
    "cuts = vegasFileIO.loadTheCutsInfo()\n",
    "\n",
    "startTime = runHeader.getStartTime()\n",
    "endTime = runHeader.getEndTime()\n",
    "\n",
    "startTime_s = float(startTime.getDayNS()) / 1e9\n",
    "endTime_s = float(endTime.getDayNS()) / 1e9\n",
    "\n",
    "# Get Time Cuts config string\n",
    "for k in cuts:\n",
    "    tmp =k.fCutsFileText\n",
    "    tc = getTimeCut(k.fCutsFileText)\n",
    "    \n",
    "print('Time Cuts:',mergeTimeCut(tc))\n",
    "goodTimeStart,goodTimeStop = getGTArray(startTime_s,endTime_s,mergeTimeCut(tc))\n",
    "print('Start Time array',goodTimeStart)\n",
    "print('Stop Time array',goodTimeStop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do we record the unit in this table as well? (it is in seconds)\n",
    "\n",
    "hdu2 = fits.BinTableHDU.from_columns([\n",
    "    fits.Column(name='START', format='1D', array=goodTimeStart), \n",
    "    fits.Column(name='STOP', format='1D', array=goodTimeStop)\n",
    "])\n",
    "hdu2.name = \"GTI\"\n",
    "hdu2.header.set('TSTART',startTime_s,'start time same unit and system used in the rate table')\n",
    "hdu2.header.set('TSTOP',endTime_s,'stop time same unit and system used in the rate table')\n",
    "hdu2.header.set('TIMEZERO',startTime_s,'zero time same unit and system used in the rate table') # is this correct ? \n",
    "hdu2.header.set('TTYPE1','START   ' ,' start of good time interval')\n",
    "hdu2.header.set('TTYPE2','STOP    ' ,' start of good time interval')\n",
    "hdu2.header.set('EXTNAME','GTI     ' ,' name: Good Time Intervals')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effective Area\n",
    "\n",
    "Question - what do we need to do about sim vs real spectral index?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First we need to load the EA file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "effectiveAreaIO = VARootIO(eaFile, True)\n",
    "effectiveAreaIO.loadTheRootFile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then we need to load the Effective Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "effectiveAreaManager = VAEffectiveAreaManager()\n",
    "effectiveAreaManager.loadEffectiveAreas(effectiveAreaIO)\n",
    "effectiveAreaManager.setUseReconstructedEnergy(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Average Noise of run\n",
    "\n",
    "#### How does VEGAS decide what window size for noise calculation/look up ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avNoise = 0\n",
    "nTels = 0\n",
    "for telID in decodeConfigMask(runHeader.fRunInfo.fConfigMask):\n",
    "    avNoise += qStatsData.getCameraAverageTraceVarTimeIndpt(telID-1, windowSizeForNoise, pixelData, arrayInfo)\n",
    "    nTels += 1\n",
    "\n",
    "avNoise /= nTels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the EA parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "effectiveAreaParameters = VAEASimpleParameterData()\n",
    "effectiveAreaParameters.fAzimuth = 0#(avAz)\n",
    "effectiveAreaParameters.fZenith = 20#(90. - avAlt)\n",
    "effectiveAreaParameters.fNoise = 5.3#avNoise # this still needs to be worked out\n",
    "effectiveAreaParameters.fOffset = 0.5 # itterate over this in steps? (0.5deg as for sims?)\n",
    "\n",
    "# convert to vector of parameters since this is required for a number of steps\n",
    "effectiveAreaParameters = effectiveAreaManager.getVectorParamsFromSimpleParameterData(effectiveAreaParameters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the EA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "effectiveArea = effectiveAreaManager.getEffectiveAreaCurve(effectiveAreaParameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets Check by Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, ye = [], [], []\n",
    "for i in range(effectiveArea.GetN()):\n",
    "    tmpX, tmpY = ROOT.Double(0), ROOT.Double(0)\n",
    "    effectiveArea.GetPoint(i, tmpX, tmpY)\n",
    "    ye.append(effectiveArea.GetErrorY(i))\n",
    "    effectiveArea.GetErrorX\n",
    "    x.append(tmpX)\n",
    "    y.append(tmpY)\n",
    "        \n",
    "x = np.array(x)\n",
    "y = np.array(y)\n",
    "ye = np.array(ye)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this can be removed from the later code but is good for checking\n",
    "plt.errorbar(x, y, yerr = ye, ls = \"\", marker = \"_\")\n",
    "plt.semilogy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_float(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energyLow = np.power(10, x - (x[1] - x[0])/2.)\n",
    "energyHigh = np.power(10, x + (x[1] - x[0])/2.)\n",
    "thetaLow = [0.0, 1.0]\n",
    "thetaHigh = [1.0, 2.0]\n",
    "# ea = np.vstack((y, y))\n",
    "ea = [y,y]\n",
    "minEnergy , maxEnergy = c_float(), c_float()\n",
    "effectiveAreaManager.getSafeEnergyRange(effectiveAreaParameters, 0.5, minEnergy, maxEnergy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([(energyLow, energyHigh, thetaLow, thetaHigh, ea)], \n",
    "             dtype=[('ENERG_LO', '>f4', np.shape(energyLow)), \n",
    "                    ('ENERG_HI', '>f4', np.shape(energyHigh)), \n",
    "                    ('THETA_LO', '>f4', np.shape(thetaLow)), \n",
    "                    ('THETA_HI', '>f4', np.shape(thetaHigh)), \n",
    "                    ('EFFAREA', '>f4', np.shape(ea))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdu3 = fits.BinTableHDU(data=x)\n",
    "hdu3.name = \"EFFECTIVE AREA\"\n",
    "\n",
    "hdu3.header.set('TUNIT1 ', 'TeV', \"\")\n",
    "hdu3.header.set('TUNIT2 ', 'TeV', \"\")\n",
    "hdu3.header.set('TUNIT3 ', 'deg', \"\")\n",
    "hdu3.header.set('TUNIT4 ', 'deg', \"\")\n",
    "hdu3.header.set('TUNIT5 ', 'm^2', \"\")\n",
    "\n",
    "hdu3.header.set('HDUCLASS', 'GADF',\n",
    "                'FITS file following the GADF data format.')\n",
    "hdu3.header.set('HDUCLAS1', 'RESPONSE', 'HDU class')\n",
    "hdu3.header.set('HDUCLAS2', 'EFF_AREA', 'HDU class')\n",
    "hdu3.header.set('HDUCLAS3', 'POINT-LIKE', 'HDU class')\n",
    "hdu3.header.set('HDUCLAS4', 'AEFF_2D', 'HDU class')\n",
    "hdu3.header.set('LO_THRES', minEnergy.value/1000.,\n",
    "                'Low energy threshold of validity [TeV]')\n",
    "hdu3.header.set('HI_THRES', maxEnergy.value/1000.,\n",
    "                'High energy threshold of validity [TeV]')\n",
    "hdu3.header.set('RAD_MAX ', 0.1, 'Direction cut applied [deg]')\n",
    "\n",
    "hdu3.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdu3.header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Migration Matrix\n",
    "We want to get pfEnergy_Rec_VS_MC_2D for the correct parameters.\n",
    "\n",
    "This could be done with a getter within VEGAS - lets try that first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#effectiveAreaManager.getEnergyBias2D(effectiveAreaParameters)\n",
    "a, e = hist2array(effectiveAreaManager.getEnergyBias2D(effectiveAreaParameters), return_edges=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eLow = np.power(10, [e[0][:-1]])[0]\n",
    "eHigh = np.power(10, [e[0][1:]])[0]\n",
    "\n",
    "bLow = np.power(10, [e[1][:-1]])[0]\n",
    "bHigh = np.power(10, [e[1][1:]])[0]\n",
    "\n",
    "ac = []\n",
    "for aa in a:\n",
    "    if np.sum(aa) > 0:\n",
    "        ab = aa / np.sum(aa*(bHigh - bLow))\n",
    "    else:\n",
    "        ab = aa\n",
    "    try:\n",
    "        ac = np.vstack((ac, ab))\n",
    "    except:\n",
    "        ac = ab\n",
    "        \n",
    "ac = ac.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# again good for checking - will delete later\n",
    "plt.imshow(np.log10(ac))\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([(eLow, eHigh, bLow, bHigh, [0, 1.0], [1.0, 2.0], [ac, ac])], \n",
    "             dtype=[('ETRUE_LO', '>f4', (len(eLow),)), \n",
    "                    ('ETRUE_HI', '>f4', (len(eHigh),)), \n",
    "                    ('MIGRA_LO', '>f4', (len(bLow),)), \n",
    "                    ('MIGRA_HI', '>f4', (len(bLow),)), \n",
    "                    ('THETA_LO', '>f4', (2,)), \n",
    "                    ('THETA_HI', '>f4', (2,)), \n",
    "                    ('MATRIX', '>f4', (2, np.shape(ac)[0], np.shape(ac)[1]))])\n",
    "\n",
    "hdu4 = fits.BinTableHDU(data=x)\n",
    "hdu4.name = \"ENERGY DISPERSION\"\n",
    "\n",
    "hdu4.header.set('TUNIT1 ', 'TeV', \"\")\n",
    "hdu4.header.set('TUNIT2 ', 'TeV', \"\")\n",
    "hdu4.header.set('TUNIT5 ', 'deg', \"\")\n",
    "hdu4.header.set('TUNIT6 ', 'deg', \"\")\n",
    "# hdu3.header.set('TUNIT5 ', 'm^2', \"\")\n",
    "\n",
    "hdu4.header.set('HDUCLASS', 'GADF',\n",
    "                'FITS file following the GADF data format.')\n",
    "hdu4.header.set('HDUCLAS1', 'RESPONSE', 'HDU class')\n",
    "hdu4.header.set('HDUCLAS2', 'EDISP', 'HDU class')\n",
    "hdu4.header.set('HDUCLAS3', 'POINT-LIKE', 'HDU class')\n",
    "hdu4.header.set('HDUCLAS4', 'EDISP_2D', 'HDU class')\n",
    "hdu4.header.set('LO_THRES', minEnergy.value/1000.,\n",
    "                'Low energy threshold of validity [TeV]')\n",
    "hdu4.header.set('HI_THRES', maxEnergy.value/1000.,\n",
    "                'High energy threshold of validity [TeV]')\n",
    "hdu4.header.set('RAD_MAX ', 0.1, 'Direction cut applied [deg]')\n",
    "\n",
    "hdu4.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdu4.header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write FITS file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdulist = fits.HDUList([hdu0, hdu1, hdu2, hdu3, hdu4])\n",
    "hdulist.writeto(outfile, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
